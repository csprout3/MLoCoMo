{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce17540",
   "metadata": {},
   "source": [
    "## MLOCOMO: TED Talk Processing for Multimodal Memory Evaluation\n",
    "\n",
    "This notebook processes BrenÃ© Brown's \"The Power of Vulnerability\" TED talk into MLOCOMO format.\n",
    "\n",
    " **What you'll get:**\n",
    " - ~74 video segments with multimodal annotations\n",
    " - ~45 QA pairs across all recall types and modality combinations  \n",
    " - Complete dataset ready for memory evaluation\n",
    " \n",
    "**Estimated time:** 15-20 minutes\n",
    "**Estimated cost:** ~$3 in OpenAI API calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai\n",
    "!pip install -q openai-whisper\n",
    "!pip install -q yt-dlp\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b39905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import time\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import whisper\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video, Audio, Image, HTML, display, clear_output\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7db5e",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OPENAI_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Validate API key\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"âš ï¸  Please enter your OpenAI API key above and run this cell again\")\n",
    "    print(\"Get your API key from: https://platform.openai.com/api-keys\")\n",
    "else:\n",
    "    # Test API key\n",
    "    try:\n",
    "        client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "        # Simple test call\n",
    "        test_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"âœ… OpenAI API key validated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API key validation failed: {e}\")\n",
    "        OPENAI_API_KEY = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6c388",
   "metadata": {},
   "source": [
    "## Step 3: Load Models and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLOCOMOProcessor:\n",
    "    \"\"\"MLOCOMO processor optimized for Google Colab\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.setup_complete = False\n",
    "        \n",
    "        # TED Talk configuration\n",
    "        self.video_config = {\n",
    "            \"url\": \"https://www.youtube.com/watch?v=iCvmsMzlF7o\",\n",
    "            \"title\": \"BrenÃ© Brown: The Power of Vulnerability\",\n",
    "            \"duration\": 1260,  # ~21 minutes\n",
    "            \"license\": \"CC BY-NC-ND 4.0\"\n",
    "        }\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Setup models with progress tracking\"\"\"\n",
    "        print(\"ðŸ”„ Setting up models...\")\n",
    "        \n",
    "        # Load Whisper\n",
    "        try:\n",
    "            print(\"Loading Whisper model...\")\n",
    "            self.whisper_model = whisper.load_model(\"base\")\n",
    "            print(\"âœ… Whisper loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Whisper failed, will use OpenAI API: {e}\")\n",
    "            self.whisper_model = None\n",
    "        \n",
    "        # Load sentence transformer\n",
    "        try:\n",
    "            print(\"Loading sentence transformer...\")\n",
    "            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(\"âœ… Sentence transformer loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Sentence transformer failed: {e}\")\n",
    "            self.sentence_model = None\n",
    "        \n",
    "        self.setup_complete = True\n",
    "        print(\"ðŸŽ‰ Setup complete!\")\n",
    "    \n",
    "    def download_video(self) -> str:\n",
    "        \"\"\"Download TED talk video\"\"\"\n",
    "        print(\"ðŸ“¥ Downloading TED talk...\")\n",
    "        \n",
    "        output_path = \"ted_talk.mp4\"\n",
    "        \n",
    "        cmd = [\n",
    "            \"yt-dlp\",\n",
    "            \"--format\", \"best[height<=720][ext=mp4]/best[ext=mp4]\",\n",
    "            \"--output\", output_path,\n",
    "            self.video_config[\"url\"]\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Show progress\n",
    "            process = subprocess.Popen(\n",
    "                cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                universal_newlines=True, bufsize=1\n",
    "            )\n",
    "            \n",
    "            for line in process.stdout:\n",
    "                if \"%\" in line and \"ETA\" in line:\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"ðŸ“¥ Downloading: {line.strip()}\")\n",
    "            \n",
    "            process.wait()\n",
    "            \n",
    "            if process.returncode == 0 and os.path.exists(output_path):\n",
    "                print(\"âœ… Download complete!\")\n",
    "                return output_path\n",
    "            else:\n",
    "                raise RuntimeError(\"Download failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Download failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def extract_audio(self, video_path: str) -> str:\n",
    "        \"\"\"Extract audio from video\"\"\"\n",
    "        print(\"ðŸŽµ Extracting audio...\")\n",
    "        \n",
    "        audio_path = \"audio.wav\"\n",
    "        \n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-i\", video_path, \"-vn\", \n",
    "            \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\",\n",
    "            audio_path, \"-y\", \"-loglevel\", \"quiet\"\n",
    "        ]\n",
    "        \n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(\"âœ… Audio extracted!\")\n",
    "        return audio_path\n",
    "    \n",
    "    async def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Transcribe audio with timestamps\"\"\"\n",
    "        print(\"ðŸ—£ï¸  Transcribing audio...\")\n",
    "        \n",
    "        if self.whisper_model:\n",
    "            print(\"Using local Whisper...\")\n",
    "            result = self.whisper_model.transcribe(\n",
    "                audio_path, word_timestamps=True, language=\"en\"\n",
    "            )\n",
    "            print(\"âœ… Local transcription complete!\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"Using OpenAI Whisper API...\")\n",
    "            with open(audio_path, \"rb\") as f:\n",
    "                transcript = await self.client.audio.transcriptions.acreate(\n",
    "                    model=\"whisper-1\", file=f, response_format=\"verbose_json\"\n",
    "                )\n",
    "            \n",
    "            result = {\n",
    "                \"text\": transcript.text,\n",
    "                \"segments\": []\n",
    "            }\n",
    "            \n",
    "            if hasattr(transcript, 'segments'):\n",
    "                for seg in transcript.segments:\n",
    "                    result[\"segments\"].append({\n",
    "                        \"start\": seg.start,\n",
    "                        \"end\": seg.end,\n",
    "                        \"text\": seg.text\n",
    "                    })\n",
    "            \n",
    "            print(\"âœ… API transcription complete!\")\n",
    "            return result\n",
    "    \n",
    "    def identify_speakers(self, transcription: Dict) -> List[Dict]:\n",
    "        \"\"\"Simple speaker identification\"\"\"\n",
    "        print(\"ðŸ‘¥ Identifying speakers...\")\n",
    "        \n",
    "        segments = transcription.get(\"segments\", [])\n",
    "        speaker_segments = []\n",
    "        \n",
    "        for segment in tqdm(segments, desc=\"Processing segments\"):\n",
    "            text = segment.get(\"text\", \"\").lower().strip()\n",
    "            start_time = segment.get(\"start\", 0)\n",
    "            \n",
    "            # Simple heuristics\n",
    "            if text.endswith(\"?\") or len(text.split()) < 8:\n",
    "                speaker_id = \"audience_member\"\n",
    "                confidence = 0.7\n",
    "            elif start_time < 60:\n",
    "                speaker_id = \"moderator\" \n",
    "                confidence = 0.6\n",
    "            else:\n",
    "                speaker_id = \"brene_brown\"\n",
    "                confidence = 0.8\n",
    "            \n",
    "            speaker_segments.append({\n",
    "                **segment,\n",
    "                \"speaker_id\": speaker_id,\n",
    "                \"speaker_confidence\": confidence\n",
    "            })\n",
    "        \n",
    "        print(\"âœ… Speaker identification complete!\")\n",
    "        return speaker_segments\n",
    "    \n",
    "    def create_segments(self, video_path: str, speaker_segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create 30-second segments with basic analysis\"\"\"\n",
    "        print(\"ðŸŽ¬ Creating video segments...\")\n",
    "        \n",
    "        # Group into 30-second segments\n",
    "        segments = []\n",
    "        current_group = []\n",
    "        current_duration = 0\n",
    "        target_duration = 30.0\n",
    "        \n",
    "        for seg in speaker_segments:\n",
    "            seg_duration = seg[\"end\"] - seg[\"start\"]\n",
    "            \n",
    "            if current_duration + seg_duration > target_duration and current_group:\n",
    "                # Process current group\n",
    "                segment = self._process_segment_group(video_path, current_group, len(segments))\n",
    "                if segment:\n",
    "                    segments.append(segment)\n",
    "                \n",
    "                # Start new group\n",
    "                current_group = [seg]\n",
    "                current_duration = seg_duration\n",
    "            else:\n",
    "                current_group.append(seg)\n",
    "                current_duration += seg_duration\n",
    "        \n",
    "        # Process final group\n",
    "        if current_group:\n",
    "            segment = self._process_segment_group(video_path, current_group, len(segments))\n",
    "            if segment:\n",
    "                segments.append(segment)\n",
    "        \n",
    "        print(f\"âœ… Created {len(segments)} segments!\")\n",
    "        return segments\n",
    "    \n",
    "    def _process_segment_group(self, video_path: str, group: List[Dict], segment_idx: int) -> Optional[Dict]:\n",
    "        \"\"\"Process a group of transcription segments into one video segment\"\"\"\n",
    "        \n",
    "        if not group:\n",
    "            return None\n",
    "        \n",
    "        start_time = group[0][\"start\"]\n",
    "        end_time = group[-1][\"end\"]\n",
    "        \n",
    "        # Skip if too short\n",
    "        if end_time - start_time < 10:\n",
    "            return None\n",
    "        \n",
    "        # Combine transcript\n",
    "        transcript = \" \".join([seg[\"text\"] for seg in group])\n",
    "        \n",
    "        # Determine speaker\n",
    "        speakers = [seg[\"speaker_id\"] for seg in group]\n",
    "        primary_speaker = max(set(speakers), key=speakers.count)\n",
    "        speaker_confidence = np.mean([seg[\"speaker_confidence\"] for seg in group])\n",
    "        \n",
    "        # Extract frame for analysis\n",
    "        frame = self._extract_frame(video_path, (start_time + end_time) / 2)\n",
    "        \n",
    "        return {\n",
    "            \"segment_id\": f\"seg_{segment_idx:03d}\",\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"transcript\": transcript,\n",
    "            \"speaker_id\": primary_speaker,\n",
    "            \"speaker_confidence\": speaker_confidence,\n",
    "            \"frame\": frame,  # Store for later analysis\n",
    "            \"visual_description\": \"\",  # Will be filled later\n",
    "            \"audio_description\": \"\",   # Will be filled later\n",
    "            \"key_objects\": [],         # Will be filled later\n",
    "            \"scene_type\": \"presentation\"\n",
    "        }\n",
    "    \n",
    "    def _extract_frame(self, video_path: str, timestamp: float) -> Optional[np.ndarray]:\n",
    "        \"\"\"Extract frame at timestamp\"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_number = int(timestamp * fps)\n",
    "            \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            \n",
    "            return frame if ret else None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    async def analyze_segments(self, segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Analyze all segments with multimodal content\"\"\"\n",
    "        print(\"ðŸ” Analyzing multimodal content...\")\n",
    "        \n",
    "        for i, segment in enumerate(tqdm(segments, desc=\"Analyzing segments\")):\n",
    "            # Visual analysis\n",
    "            if segment[\"frame\"] is not None:\n",
    "                segment[\"visual_description\"] = await self._analyze_visual(\n",
    "                    segment[\"frame\"], segment[\"transcript\"]\n",
    "                )\n",
    "                \n",
    "                # Extract key objects\n",
    "                segment[\"key_objects\"] = await self._extract_key_objects(\n",
    "                    segment[\"visual_description\"], segment[\"transcript\"]\n",
    "                )\n",
    "            \n",
    "            # Audio analysis\n",
    "            segment[\"audio_description\"] = await self._analyze_audio(\n",
    "                segment[\"transcript\"], segment[\"start_time\"], segment[\"end_time\"]\n",
    "            )\n",
    "            \n",
    "            # Remove frame data (too large for JSON)\n",
    "            del segment[\"frame\"]\n",
    "            \n",
    "            # Progress update every 10 segments\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"ðŸ” Analyzed {i}/{len(segments)} segments...\")\n",
    "        \n",
    "        print(\"âœ… Multimodal analysis complete!\")\n",
    "        return segments\n",
    "    \n",
    "    async def _analyze_visual(self, frame: np.ndarray, transcript: str) -> str:\n",
    "        \"\"\"Analyze visual content using GPT-4V\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Encode frame\n",
    "            _, buffer = cv2.imencode('.jpg', frame)\n",
    "            frame_b64 = base64.b64encode(buffer).decode('utf-8')\n",
    "            \n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"Describe this TED talk frame. Speaker says: '{transcript[:150]}...' \"\n",
    "                                   f\"Focus on: setting, speaker, gestures, slides, audience. Be concise.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame_b64}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                }],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            return \"Visual analysis unavailable\"\n",
    "    \n",
    "    async def _analyze_audio(self, transcript: str, start_time: float, end_time: float) -> str:\n",
    "        \"\"\"Analyze audio characteristics\"\"\"\n",
    "        \n",
    "        try:\n",
    "            duration = end_time - start_time\n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Describe audio characteristics of this {duration:.1f}s TED talk segment: \"\n",
    "                              f\"'{transcript}' Focus on: pace, tone, emphasis, audience reaction.\"\n",
    "                }],\n",
    "                max_tokens=100\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            return \"Audio analysis unavailable\"\n",
    "    \n",
    "    async def _extract_key_objects(self, visual_desc: str, transcript: str) -> List[str]:\n",
    "        \"\"\"Extract key objects\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Extract 3-5 key objects/concepts: Visual: {visual_desc} \"\n",
    "                              f\"Speech: {transcript} Return JSON list of concrete items.\"\n",
    "                }],\n",
    "                max_tokens=80\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except:\n",
    "            return [\"speaker\", \"stage\", \"audience\"]\n",
    "    \n",
    "    async def generate_qa_pairs(self, segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate QA pairs for all recall types and modalities\"\"\"\n",
    "        print(\"â“ Generating QA pairs...\")\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\".join([\n",
    "            f\"Segment {i}: {seg['transcript'][:100]}... \"\n",
    "            f\"[Visual: {seg['visual_description'][:60]}...] \"\n",
    "            f\"[Speaker: {seg['speaker_id']}]\"\n",
    "            for i, seg in enumerate(segments[:8])\n",
    "        ])\n",
    "        \n",
    "        qa_pairs = []\n",
    "        recall_types = [\"single_hop\", \"multi_hop\", \"temporal\", \"commonsense\", \"adversarial\"]\n",
    "        modality_pairs = [\n",
    "            (\"text\", \"text\"), (\"audio\", \"audio\"), (\"image\", \"image\"),\n",
    "            (\"text\", \"audio\"), (\"text\", \"image\"), (\"audio\", \"text\"),\n",
    "            (\"audio\", \"image\"), (\"image\", \"text\"), (\"image\", \"audio\")\n",
    "        ]\n",
    "        \n",
    "        total_pairs = len(recall_types) * len(modality_pairs)\n",
    "        \n",
    "        with tqdm(total=total_pairs, desc=\"Generating QA pairs\") as pbar:\n",
    "            for recall_type in recall_types:\n",
    "                for source_mod, target_mod in modality_pairs:\n",
    "                    qa_pair = await self._generate_single_qa(\n",
    "                        segments, context, recall_type, source_mod, target_mod\n",
    "                    )\n",
    "                    if qa_pair:\n",
    "                        qa_pairs.append(qa_pair)\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        print(f\"âœ… Generated {len(qa_pairs)} QA pairs!\")\n",
    "        return qa_pairs\n",
    "    \n",
    "    async def _generate_single_qa(self, segments: List[Dict], context: str,\n",
    "                                recall_type: str, source_mod: str, target_mod: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate single QA pair\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Generate a {recall_type} question for BrenÃ© Brown's TED talk testing {source_mod}â†’{target_mod} memory.\n",
    "        \n",
    "        Context: {context[:1000]}...\n",
    "        \n",
    "        Guidelines:\n",
    "        - {recall_type}: {self._get_recall_description(recall_type)}\n",
    "        - Question tests {source_mod} input requiring {target_mod} knowledge\n",
    "        - Based on vulnerability research content\n",
    "        \n",
    "        JSON format:\n",
    "        {{\"question\": \"specific question\", \"answer\": \"accurate answer\", \"difficulty\": \"easy|medium|hard\"}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            qa_data = json.loads(response.choices[0].message.content)\n",
    "            qa_data.update({\n",
    "                \"recall_type\": recall_type,\n",
    "                \"source_modality\": source_mod,\n",
    "                \"target_modality\": target_mod,\n",
    "                \"requires_cross_modal\": source_mod != target_mod\n",
    "            })\n",
    "            \n",
    "            return qa_data\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _get_recall_description(self, recall_type: str) -> str:\n",
    "        descriptions = {\n",
    "            \"single_hop\": \"Direct factual recall from one moment\",\n",
    "            \"multi_hop\": \"Connect information across multiple segments\", \n",
    "            \"temporal\": \"Time-based reasoning about sequence/duration\",\n",
    "            \"commonsense\": \"Inference requiring background knowledge\",\n",
    "            \"adversarial\": \"Edge cases or challenging scenarios\"\n",
    "        }\n",
    "        return descriptions.get(recall_type, \"\")\n",
    "\n",
    "# Initialize processor\n",
    "if OPENAI_API_KEY:\n",
    "    processor = MLOCOMOProcessor(OPENAI_API_KEY)\n",
    "    processor.setup_models()\n",
    "else:\n",
    "    print(\"âŒ Cannot proceed without valid API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198685c8",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 4: Download and Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a49b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TED talk\n",
    "if OPENAI_API_KEY and processor.setup_complete:\n",
    "    try:\n",
    "        video_path = processor.download_video()\n",
    "        \n",
    "        # Show video info\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        duration = frame_count / fps\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap.release()\n",
    "        \n",
    "        print(f\"ðŸ“Š Video Info:\")\n",
    "        print(f\"   Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
    "        print(f\"   Resolution: {width}x{height}\")\n",
    "        print(f\"   FPS: {fps:.1f}\")\n",
    "        print(f\"   Size: {os.path.getsize(video_path)/1024/1024:.1f} MB\")\n",
    "        \n",
    "        # Display first frame\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(frame_rgb)\n",
    "            plt.title(\"First Frame - BrenÃ© Brown TED Talk\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Video download failed: {e}\")\n",
    "        video_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edfa07",
   "metadata": {},
   "source": [
    "## ðŸŽµ Step 5: Audio Processing and Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb05a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if video_path:\n",
    "    try:\n",
    "        # Extract audio\n",
    "        audio_path = processor.extract_audio(video_path)\n",
    "        \n",
    "        # Transcribe\n",
    "        transcription = await processor.transcribe_audio(audio_path)\n",
    "        \n",
    "        print(f\"ðŸ“Š Transcription Info:\")\n",
    "        print(f\"   Total text length: {len(transcription['text'])} characters\")\n",
    "        print(f\"   Number of segments: {len(transcription.get('segments', []))}\")\n",
    "        \n",
    "        # Show sample transcription\n",
    "        print(f\"\\nðŸ“ Sample transcription:\")\n",
    "        print(f\"'{transcription['text'][:300]}...'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Audio processing failed: {e}\")\n",
    "        audio_path = None\n",
    "        transcription = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07913a3e",
   "metadata": {},
   "source": [
    "## ðŸ‘¥ Step 6: Speaker Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae198cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transcription:\n",
    "    try:\n",
    "        speaker_segments = processor.identify_speakers(transcription)\n",
    "        \n",
    "        # Analyze speaker distribution\n",
    "        speakers = [seg[\"speaker_id\"] for seg in speaker_segments]\n",
    "        speaker_counts = {\n",
    "            speaker: speakers.count(speaker) \n",
    "            for speaker in set(speakers)\n",
    "        }\n",
    "        \n",
    "        print(\"ðŸŽ¯ Speaker Distribution:\")\n",
    "        for speaker, count in speaker_counts.items():\n",
    "            percentage = (count / len(speakers)) * 100\n",
    "            print(f\"   {speaker}: {count} segments ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Visualize speaker timeline\n",
    "        fig, ax = plt.subplots(figsize=(15, 4))\n",
    "        \n",
    "        colors = {'brene_brown': 'blue', 'moderator': 'green', 'audience_member': 'red'}\n",
    "        \n",
    "        for seg in speaker_segments:\n",
    "            speaker = seg[\"speaker_id\"]\n",
    "            start = seg[\"start\"]\n",
    "            duration = seg[\"end\"] - seg[\"start\"]\n",
    "            ax.barh(speaker, duration, left=start, color=colors.get(speaker, 'gray'), alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_title('Speaker Timeline')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Speaker identification failed: {e}\")\n",
    "        speaker_segments = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c8934",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Step 7: Create Video Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e84a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if speaker_segments:\n",
    "    try:\n",
    "        segments = processor.create_segments(video_path, speaker_segments)\n",
    "        \n",
    "        print(f\"ðŸ“Š Segment Statistics:\")\n",
    "        print(f\"   Total segments: {len(segments)}\")\n",
    "        \n",
    "        if segments:\n",
    "            durations = [seg[\"end_time\"] - seg[\"start_time\"] for seg in segments]\n",
    "            print(f\"   Average duration: {np.mean(durations):.1f} seconds\")\n",
    "            print(f\"   Duration range: {np.min(durations):.1f} - {np.max(durations):.1f} seconds\")\n",
    "            \n",
    "            # Show segment distribution\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Duration histogram\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(durations, bins=20, alpha=0.7, color='skyblue')\n",
    "            plt.xlabel('Segment Duration (seconds)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Segment Duration Distribution')\n",
    "            \n",
    "            # Speaker distribution in segments\n",
    "            plt.subplot(1, 2, 2)\n",
    "            segment_speakers = [seg[\"speaker_id\"] for seg in segments]\n",
    "            speaker_counts = {s: segment_speakers.count(s) for s in set(segment_speakers)}\n",
    "            plt.pie(speaker_counts.values(), labels=speaker_counts.keys(), autopct='%1.1f%%')\n",
    "            plt.title('Speaker Distribution in Segments')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        print(\"âœ… Video segmentation complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Segmentation failed: {e}\")\n",
    "        segments = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddebe81",
   "metadata": {},
   "source": [
    "## ðŸ” Step 8: Multimodal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if segments:\n",
    "    try:\n",
    "        print(\"ðŸ”„ Starting multimodal analysis...\")\n",
    "        print(\"â±ï¸  This will take 5-10 minutes...\")\n",
    "        \n",
    "        analyzed_segments = await processor.analyze_segments(segments)\n",
    "        \n",
    "        print(f\"âœ… Multimodal analysis complete!\")\n",
    "        print(f\"   Analyzed {len(analyzed_segments)} segments\")\n",
    "        \n",
    "        # Show sample analysis\n",
    "        if analyzed_segments:\n",
    "            sample = analyzed_segments[0]\n",
    "            print(f\"\\nðŸ“‹ Sample Analysis (Segment 0):\")\n",
    "            print(f\"   Transcript: {sample['transcript'][:100]}...\")\n",
    "            print(f\"   Visual: {sample['visual_description'][:100]}...\")\n",
    "            print(f\"   Audio: {sample['audio_description'][:100]}...\")\n",
    "            print(f\"   Key Objects: {sample['key_objects']}\")\n",
    "            print(f\"   Speaker: {sample['speaker_id']} ({sample['speaker_confidence']:.2f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Multimodal analysis failed: {e}\")\n",
    "        analyzed_segments = segments  # Use basic segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7467740",
   "metadata": {},
   "source": [
    "## â“ Step 9: Generate QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyzed_segments:\n",
    "    try:\n",
    "        print(\"ðŸ”„ Generating QA pairs...\")\n",
    "        print(\"â±ï¸  This will take 8-12 minutes...\")\n",
    "        \n",
    "        qa_pairs = await processor.generate_qa_pairs(analyzed_segments)\n",
    "        \n",
    "        print(f\"âœ… QA generation complete!\")\n",
    "        print(f\"   Generated {len(qa_pairs)} QA pairs\")\n",
    "        \n",
    "        # Analyze QA distribution\n",
    "        recall_types = [qa[\"recall_type\"] for qa in qa_pairs]\n",
    "        modality_pairs = [f\"{qa['source_modality']}â†’{qa['target_modality']}\" for qa in qa_pairs]\n",
    "        \n",
    "        print(f\"\\nðŸ“Š QA Distribution:\")\n",
    "        \n",
    "        # Recall type distribution\n",
    "        recall_counts = {rt: recall_types.count(rt) for rt in set(recall_types)}\n",
    "        print(f\"   By recall type:\")\n",
    "        for rt, count in recall_counts.items():\n",
    "            print(f\"      {rt}: {count}\")\n",
    "        \n",
    "        # Cross-modal vs unimodal\n",
    "        cross_modal = sum(1 for qa in qa_pairs if qa[\"requires_cross_modal\"])\n",
    "        unimodal = len(qa_pairs) - cross_modal\n",
    "        print(f\"   Cross-modal: {cross_modal}, Unimodal: {unimodal}\")\n",
    "        \n",
    "        # Show sample QA pairs\n",
    "        print(f\"\\nðŸ“ Sample QA Pairs:\")\n",
    "        for i, qa in enumerate(qa_pairs[:3]):\n",
    "            print(f\"   {i+1}. [{qa['recall_type']}] {qa['question']}\")\n",
    "            print(f\"      Answer: {qa['answer']}\")\n",
    "            print(f\"      Modality: {qa['source_modality']}â†’{qa['target_modality']}\")\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ QA generation failed: {e}\")\n",
    "        qa_pairs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76215ce",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723454b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyzed_segments and qa_pairs:\n",
    "    try:\n",
    "        # Create final dataset\n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"source_video\": processor.video_config[\"url\"],\n",
    "                \"title\": processor.video_config[\"title\"],\n",
    "                \"license\": processor.video_config[\"license\"],\n",
    "                \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_segments\": len(analyzed_segments),\n",
    "                \"total_qa_pairs\": len(qa_pairs),\n",
    "                \"processing_tool\": \"Google Colab MLOCOMO Processor\"\n",
    "            },\n",
    "            \"segments\": analyzed_segments,\n",
    "            \"qa_pairs\": qa_pairs\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        output_filename = \"brene_brown_mlocomo_dataset.json\"\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "        \n",
    "        # Calculate file size\n",
    "        file_size = os.path.getsize(output_filename) / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"ðŸ’¾ Dataset saved successfully!\")\n",
    "        print(f\"   Filename: {output_filename}\")\n",
    "        print(f\"   File size: {file_size:.1f} MB\")\n",
    "        print(f\"   Segments: {len(analyzed_segments)}\")\n",
    "        print(f\"   QA pairs: {len(qa_pairs)}\")\n",
    "        \n",
    "        # Create summary report\n",
    "        summary = f\"\"\"\n",
    "MLOCOMO Dataset Processing Report\n",
    "================================\n",
    "\n",
    "Video Information:\n",
    "- Title: {dataset['metadata']['title']}\n",
    "- Source: {dataset['metadata']['source_video']}\n",
    "- License: {dataset['metadata']['license']}\n",
    "- Processing Date: {dataset['metadata']['processing_timestamp']}\n",
    "\n",
    "Dataset Statistics:\n",
    "- Total Segments: {len(analyzed_segments)}\n",
    "- Total QA Pairs: {len(qa_pairs)}\n",
    "- File Size: {file_size:.1f} MB\n",
    "\n",
    "Speaker Distribution:\n",
    "\"\"\"\n",
    "        \n",
    "        # Add speaker distribution\n",
    "        speakers = [seg[\"speaker_id\"] for seg in analyzed_segments]\n",
    "        for speaker in set(speakers):\n",
    "            count = speakers.count(speaker)\n",
    "            percentage = (count / len(speakers)) * 100\n",
    "            summary += f\"- {speaker}: {count} segments ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "QA Pair Distribution:\n",
    "\"\"\"\n",
    "        \n",
    "        # Add QA distribution\n",
    "        for recall_type in set(recall_types):\n",
    "            count = recall_types.count(recall_type)\n",
    "            summary += f\"- {recall_type}: {count}\\n\"\n",
    "        \n",
    "        # Save summary\n",
    "        with open(\"processing_summary.txt\", 'w') as f:\n",
    "            f.write(summary)\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ Summary report saved to: processing_summary.txt\")\n",
    "        \n",
    "        # Display download links\n",
    "        from google.colab import files\n",
    "        \n",
    "        print(f\"\\nðŸ“¥ Download your files:\")\n",
    "        print(f\"   1. Main dataset: {output_filename}\")\n",
    "        print(f\"   2. Summary report: processing_summary.txt\")\n",
    "        \n",
    "        # Auto-download files\n",
    "        files.download(output_filename)\n",
    "        files.download(\"processing_summary.txt\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ MLOCOMO processing complete!\")\n",
    "        print(f\"   âœ… {len(analyzed_segments)} multimodal segments created\")\n",
    "        print(f\"   âœ… {len(qa_pairs)} QA pairs generated\")\n",
    "        print(f\"   âœ… Ready for memory evaluation experiments!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Save failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59721c3",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 11: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyzed_segments and qa_pairs:\n",
    "    \n",
    "    # 1. Segment timeline with multimodal content\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
    "    \n",
    "    # Timeline of segments\n",
    "    for i, seg in enumerate(analyzed_segments):\n",
    "        start = seg[\"start_time\"]\n",
    "        duration = seg[\"end_time\"] - seg[\"start_time\"]\n",
    "        speaker = seg[\"speaker_id\"]\n",
    "        \n",
    "        color_map = {'brene_brown': 'blue', 'moderator': 'green', 'audience_member': 'red'}\n",
    "        color = color_map.get(speaker, 'gray')\n",
    "        \n",
    "        ax1.barh(0, duration, left=start, color=color, alpha=0.7, height=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Time (seconds)')\n",
    "    ax1.set_title('Video Segment Timeline by Speaker')\n",
    "    ax1.set_ylim(-0.5, 0.5)\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # QA pair distribution heatmap\n",
    "    recall_types = [\"single_hop\", \"multi_hop\", \"temporal\", \"commonsense\", \"adversarial\"]\n",
    "    modalities = [\"text\", \"audio\", \"image\"]\n",
    "    \n",
    "    # Create matrix for cross-modal pairs\n",
    "    matrix = np.zeros((len(modalities), len(modalities)))\n",
    "    \n",
    "    for qa in qa_pairs:\n",
    "        source_idx = modalities.index(qa[\"source_modality\"])\n",
    "        target_idx = modalities.index(qa[\"target_modality\"])\n",
    "        matrix[source_idx][target_idx] += 1\n",
    "    \n",
    "    im = ax2.imshow(matrix, cmap='Blues')\n",
    "    ax2.set_xticks(range(len(modalities)))\n",
    "    ax2.set_yticks(range(len(modalities)))\n",
    "    ax2.set_xticklabels(modalities)\n",
    "    ax2.set_yticklabels(modalities)\n",
    "    ax2.set_xlabel('Target Modality')\n",
    "    ax2.set_ylabel('Source Modality')\n",
    "    ax2.set_title('QA Pairs by Modality Transfer')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(modalities)):\n",
    "        for j in range(len(modalities)):\n",
    "            text = ax2.text(j, i, int(matrix[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Sample frame visualization\n",
    "    print(\"ðŸ–¼ï¸  Sample Frames and Analysis:\")\n",
    "    \n",
    "    if video_path and len(analyzed_segments) >= 3:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        for i, seg_idx in enumerate([0, len(analyzed_segments)//2, -1]):\n",
    "            seg = analyzed_segments[seg_idx]\n",
    "            timestamp = (seg[\"start_time\"] + seg[\"end_time\"]) / 2\n",
    "            \n",
    "            # Extract frame\n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC, timestamp * 1000)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                axes[i].imshow(frame_rgb)\n",
    "                axes[i].set_title(f\"Segment {seg_idx}: {seg['speaker_id']}\\n{seg['transcript'][:50]}...\")\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        cap.release()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"âœ… Visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e2935",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "# \n",
    "# ðŸ“Š **Dataset Overview:**\n",
    "# - **Segments:** ~74 multimodal video segments \n",
    "# - **QA Pairs:** ~45 memory evaluation questions\n",
    "# - **Modalities:** Text, Audio, Visual content\n",
    "# - **Recall Types:** Single-hop, Multi-hop, Temporal, Commonsense, Adversarial\n",
    "# \n",
    "# ðŸ”¬ **For Research:**\n",
    "# 1. **Evaluate MLLMs:** Test GPT-4V, Gemini, Claude on your QA pairs\n",
    "# 2. **Compare Memory Performance:** Use the evaluation metrics from LOCOMO paper\n",
    "# 3. **Cross-Modal Analysis:** Study how information transfers between modalities\n",
    "# 4. **Extend the Framework:** Add more videos or recall types\n",
    "# \n",
    "#  ðŸ’¡ **Integration:**\n",
    "# - Load the JSON dataset into your evaluation pipeline\n",
    "# - Use the segment timestamps for video-based evaluation\n",
    "# - Combine with the evaluation framework for systematic testing\n",
    "# \n",
    "# **Cost Summary:** ~$3 in OpenAI API calls for a complete research-grade dataset!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
