{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf6343",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/csprout3/MLoCoMo/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce17540",
   "metadata": {
    "id": "8ce17540"
   },
   "source": [
    "## MLOCOMO: TED Talk Processing for Multimodal Memory Evaluation\n",
    "\n",
    "This notebook processes Bren√© Brown's \"The Power of Vulnerability\" TED talk into MLOCOMO format.\n",
    "\n",
    " **What you'll get:**\n",
    " - ~74 video segments with multimodal annotations\n",
    " - ~45 QA pairs across all recall types and modality combinations  \n",
    " - Complete dataset ready for memory evaluation\n",
    "\n",
    "**Estimated time:** 15-20 minutes\n",
    "**Estimated cost:** ~$3 in OpenAI API calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q3wlMzJv5u8Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3wlMzJv5u8Y",
    "outputId": "81bbdba3-e984-4f4a-fa28-9c4e5d71fa0d"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECURE API KEY SETUP FOR GOOGLE COLAB\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def setup_colab_environment():\n",
    "    \"\"\"Set up environment for Google Colab with secure API key handling\"\"\"\n",
    "    \n",
    "    # Method 1: Try Colab userdata (most secure)\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('OPENAI_API_KEY')\n",
    "        if api_key:\n",
    "            os.environ['OPENAI_API_KEY'] = api_key\n",
    "            print(\"‚úÖ API key loaded from Colab userdata\")\n",
    "            return api_key\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not load from Colab userdata: {e}\")\n",
    "        print(\"ÔøΩÔøΩ To set up userdata: Click the key icon (üîë) in left sidebar\")\n",
    "    \n",
    "    # Method 2: Manual input (fallback)\n",
    "    import getpass\n",
    "    api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    print(\"‚úÖ API key set from manual input\")\n",
    "    return api_key\n",
    "\n",
    "# Set up the environment\n",
    "OPENAI_API_KEY = setup_colab_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5b116",
   "metadata": {
    "id": "d0b5b116"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai\n",
    "!pip install -q openai-whisper\n",
    "!pip install -q yt-dlp\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q tqdm\n",
    "!pip install -q opencv-python\n",
    "!pip install -q matplotlib\n",
    "!pip install -q ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b39905",
   "metadata": {
    "id": "14b39905"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import time\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import whisper\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video, Audio, Image, HTML, display, clear_output\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required packages\n",
    "required_packages = ['openai', 'whisper', 'cv2', 'sentence_transformers', 'tqdm']\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        if package == 'cv2':\n",
    "            import cv2\n",
    "        elif package == 'whisper':\n",
    "            import whisper\n",
    "        elif package == 'sentence_transformers':\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "        else:\n",
    "            __import__(package)\n",
    "        print(f\"‚úÖ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package}\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing packages: {missing_packages}\")\n",
    "    print(\"Run the pip install cell above first!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required packages are available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7db5e",
   "metadata": {
    "id": "fdc7db5e"
   },
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169c61e",
   "metadata": {
    "id": "a169c61e"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OPENAI_API_KEY = OPENAI_API_KEY\n",
    "\n",
    "# Validate API key\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  Please enter your OpenAI API key above and run this cell again\")\n",
    "    print(\"Get your API key from: https://platform.openai.com/api-keys\")\n",
    "else:\n",
    "    # Test API key\n",
    "    try:\n",
    "        client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "        # Simple test call\n",
    "        test_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"‚úÖ OpenAI API key validated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API key validation failed: {e}\")\n",
    "        OPENAI_API_KEY = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOCOMO ENHANCED PROCESSOR - Based on arXiv:2402.17753\n",
    "# =============================================================================\n",
    "\n",
    "class LOCOMOEnhancedProcessor:\n",
    "    \"\"\"Enhanced processor that creates LOCOMO-compliant multi-session conversations\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.setup_complete = False\n",
    "        \n",
    "        # LOCOMO requirements from paper\n",
    "        self.target_sessions = 35  # LOCOMO uses 35 sessions\n",
    "        self.target_turns_per_session = 8-12  # ~300 total turns\n",
    "        self.recall_types = [\n",
    "            \"single_hop\", \"multi_hop\", \"temporal\", \n",
    "            \"commonsense\", \"adversarial\", \"cross_modal\"\n",
    "        ]\n",
    "        \n",
    "        # TED Talk configuration\n",
    "        self.video_config = {\n",
    "            \"url\": \"https://www.youtube.com/watch?v=iCvmsMzlF7o\",\n",
    "            \"title\": \"Bren√© Brown: The Power of Vulnerability\",\n",
    "            \"duration\": 1260,  # ~21 minutes\n",
    "            \"license\": \"CC BY-NC-ND 4.0\"\n",
    "        }\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Load required models\"\"\"\n",
    "        print(\"üîÑ Loading models for LOCOMO processing...\")\n",
    "        \n",
    "        # Load Whisper for transcription\n",
    "        self.whisper_model = whisper.load_model(\"base\")\n",
    "        \n",
    "        # Load sentence transformer for embeddings\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        self.setup_complete = True\n",
    "        print(\"‚úÖ Models loaded successfully!\")\n",
    "    \n",
    "    def download_video(self) -> str:\n",
    "        \"\"\"Download TED talk video\"\"\"\n",
    "        print(\"üì• Downloading TED talk...\")\n",
    "        \n",
    "        output_path = \"ted_talk.mp4\"\n",
    "        \n",
    "        cmd = [\n",
    "            \"yt-dlp\",\n",
    "            \"--format\", \"best[height<=720][ext=mp4]/best[ext=mp4]\",\n",
    "            \"--output\", output_path,\n",
    "            self.video_config[\"url\"]\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            process = subprocess.Popen(\n",
    "                cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True, bufsize=1\n",
    "            )\n",
    "            \n",
    "            for line in process.stdout:\n",
    "                if \"%\" in line and \"ETA\" in line:\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"üì• Downloading: {line.strip()}\")\n",
    "            \n",
    "            process.wait()\n",
    "            \n",
    "            if process.returncode == 0 and os.path.exists(output_path):\n",
    "                print(\"‚úÖ Download complete!\")\n",
    "                return output_path\n",
    "            else:\n",
    "                raise RuntimeError(\"Download failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Download failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_personas_from_video(self, video_content: str):\n",
    "        \"\"\"Create multiple personas based on video content\"\"\"\n",
    "        \n",
    "        persona_prompt = f\"\"\"\n",
    "        Based on this TED talk content: \"{video_content[:2000]}...\"\n",
    "        \n",
    "        Create 3-4 distinct personas who would be in the audience and later discuss this talk.\n",
    "        Each persona should have different backgrounds, perspectives, and communication styles.\n",
    "        \n",
    "        Format as JSON with personas having: name, age, background, interests, \n",
    "        communication_style, relationship_to_topic, personality_traits, speech_characteristics\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": persona_prompt}],\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except:\n",
    "            # Fallback personas\n",
    "            return {\n",
    "                \"alex\": {\"name\": \"Alex\", \"age\": 28, \"background\": \"Software Engineer\", \"interests\": [\"technology\", \"personal growth\"], \"communication_style\": \"analytical\", \"relationship_to_topic\": \"professional\", \"personality_traits\": [\"curious\", \"logical\"], \"speech_characteristics\": \"speaks clearly, uses technical terms\"},\n",
    "                \"maya\": {\"name\": \"Maya\", \"age\": 35, \"background\": \"Therapist\", \"interests\": [\"psychology\", \"human behavior\"], \"communication_style\": \"empathetic\", \"relationship_to_topic\": \"professional\", \"personality_traits\": [\"compassionate\", \"intuitive\"], \"speech_characteristics\": \"speaks softly, uses metaphors\"},\n",
    "                \"jordan\": {\"name\": \"Jordan\", \"age\": 42, \"background\": \"Manager\", \"interests\": [\"leadership\", \"team building\"], \"communication_style\": \"direct\", \"relationship_to_topic\": \"professional\", \"personality_traits\": [\"confident\", \"practical\"], \"speech_characteristics\": \"speaks assertively, uses business terms\"}\n",
    "            }\n",
    "    \n",
    "    def generate_session_conversation(self, session_num: int, personas: dict, video_content: str, previous_sessions: list):\n",
    "        \"\"\"Generate a single conversation session\"\"\"\n",
    "        \n",
    "        # Build context from previous sessions\n",
    "        previous_context = \"\"\n",
    "        if previous_sessions:\n",
    "            recent_topics = [s.get('session_topic', '') for s in previous_sessions[-3:]]\n",
    "            previous_context = f\"Previous discussions covered: {', '.join(recent_topics)}\"\n",
    "        \n",
    "        # Generate conversation turns\n",
    "        conversation_prompt = f\"\"\"\n",
    "        Generate a natural conversation between these personas about the TED talk content.\n",
    "        \n",
    "        Personas: {[f\"{p['name']} ({p['background']})\" for p in personas.values()]}\n",
    "        \n",
    "        Session {session_num + 1} context: {previous_context}\n",
    "        \n",
    "        Video content to discuss: {video_content[:1000]}...\n",
    "        \n",
    "        Guidelines:\n",
    "        1. Generate 8-12 conversation turns\n",
    "        2. Each persona should speak 2-3 times\n",
    "        3. Include references to specific moments from the talk\n",
    "        4. Show different perspectives and reactions\n",
    "        5. Include personal anecdotes and connections\n",
    "        6. Maintain character consistency\n",
    "        \n",
    "        Format as JSON array of turns with speaker, text, timestamp, references_video, emotional_tone\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": conversation_prompt}],\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            turns_data = json.loads(response.choices[0].message.content)\n",
    "        except:\n",
    "            # Fallback conversation\n",
    "            turns_data = [\n",
    "                {\"speaker\": list(personas.keys())[0], \"text\": f\"Session {session_num + 1} discussion about the talk\", \"timestamp\": f\"session_{session_num + 1}\", \"references_video\": True, \"emotional_tone\": \"thoughtful\"}\n",
    "            ]\n",
    "        \n",
    "        return {\n",
    "            \"session_id\": f\"session_{session_num:03d}\",\n",
    "            \"session_number\": session_num + 1,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"participants\": list(personas.keys()),\n",
    "            \"turns\": turns_data,\n",
    "            \"session_topic\": f\"Discussion about vulnerability and leadership\",\n",
    "            \"emotional_arc\": \"progressive engagement\"\n",
    "        }\n",
    "    \n",
    "    def generate_qa_pairs(self, sessions: list, personas: dict):\n",
    "        \"\"\"Generate QA pairs following LOCOMO methodology\"\"\"\n",
    "        \n",
    "        qa_pairs = []\n",
    "        \n",
    "        # Flatten all turns for context\n",
    "        all_turns = []\n",
    "        for session in sessions:\n",
    "            all_turns.extend(session.get('turns', []))\n",
    "        \n",
    "        # Generate questions for each recall type\n",
    "        for recall_type in self.recall_types:\n",
    "            qa_prompt = f\"\"\"\n",
    "            Based on this multi-session conversation about a TED talk:\n",
    "            \n",
    "            Sessions: {len(sessions)}\n",
    "            Total turns: {len(all_turns)}\n",
    "            Participants: {list(personas.keys())}\n",
    "            \n",
    "            Generate 5 {recall_type} questions that test:\n",
    "            - Single-hop: Direct recall of specific information\n",
    "            - Multi-hop: Reasoning across multiple conversation elements\n",
    "            - Temporal: Time-based understanding and ordering\n",
    "            - Commonsense: Contextual understanding and practical reasoning\n",
    "            - Adversarial: Robustness under challenging conditions\n",
    "            - Cross-modal: Integration of different information types\n",
    "            \n",
    "            Format as JSON array with question, answer, recall_type, difficulty, requires_cross_session, evidence_turns, reasoning_steps\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": qa_prompt}],\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                \n",
    "                qa_data = json.loads(response.choices[0].message.content)\n",
    "                if isinstance(qa_data, list):\n",
    "                    qa_pairs.extend(qa_data)\n",
    "                else:\n",
    "                    qa_pairs.append(qa_data)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error generating {recall_type} questions: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return qa_pairs\n",
    "    \n",
    "    def process_video_to_locomo(self, video_path: str):\n",
    "        \"\"\"Process video into LOCOMO-compliant conversation\"\"\"\n",
    "        \n",
    "        print(\"üîÑ Processing video to LOCOMO format...\")\n",
    "        \n",
    "        # Transcribe video\n",
    "        print(\"üìù Transcribing video...\")\n",
    "        result = self.whisper_model.transcribe(video_path)\n",
    "        video_content = result[\"text\"]\n",
    "        \n",
    "        # Create personas\n",
    "        print(\"üë• Creating personas...\")\n",
    "        personas = self.create_personas_from_video(video_content)\n",
    "        print(f\"‚úÖ Created {len(personas)} personas: {list(personas.keys())}\")\n",
    "        \n",
    "        # Generate multiple sessions\n",
    "        print(f\"üí¨ Generating {self.target_sessions} conversation sessions...\")\n",
    "        sessions = []\n",
    "        for session_num in range(self.target_sessions):\n",
    "            if session_num % 5 == 0:\n",
    "                print(f\"   Session {session_num + 1}/{self.target_sessions}\")\n",
    "            \n",
    "            session = self.generate_session_conversation(session_num, personas, video_content, sessions)\n",
    "            sessions.append(session)\n",
    "        \n",
    "        # Generate QA pairs\n",
    "        print(\"‚ùì Generating QA pairs...\")\n",
    "        qa_pairs = self.generate_qa_pairs(sessions, personas)\n",
    "        \n",
    "        # Create final dataset\n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"dataset_name\": \"LOCOMO Enhanced Dataset\",\n",
    "                \"source_video\": video_path,\n",
    "                \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_sessions\": len(sessions),\n",
    "                \"total_turns\": sum(len(s.get('turns', [])) for s in sessions),\n",
    "                \"total_qa_pairs\": len(qa_pairs),\n",
    "                \"personas\": list(personas.keys()),\n",
    "                \"methodology\": \"LOCOMO Enhanced (arXiv:2402.17753)\"\n",
    "            },\n",
    "            \"personas\": personas,\n",
    "            \"sessions\": sessions,\n",
    "            \"qa_pairs\": qa_pairs\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(qa_pairs)} QA pairs!\")\n",
    "        print(f\"üìä Statistics:\")\n",
    "        print(f\"   Sessions: {len(sessions)}\")\n",
    "        print(f\"   Total turns: {sum(len(s.get('turns', [])) for s in sessions)}\")\n",
    "        print(f\"   QA pairs: {len(qa_pairs)}\")\n",
    "        print(f\"   Personas: {len(personas)}\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# Initialize enhanced processor\n",
    "if OPENAI_API_KEY:\n",
    "    enhanced_processor = LOCOMOEnhancedProcessor(OPENAI_API_KEY)\n",
    "    enhanced_processor.setup_models()\n",
    "    print(\"‚úÖ LOCOMO Enhanced Processor ready!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without valid API key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6c388",
   "metadata": {
    "id": "e6f6c388"
   },
   "source": [
    "## Step 3: Load Models and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af3fed",
   "metadata": {
    "id": "b1af3fed"
   },
   "outputs": [],
   "source": [
    "class MLOCOMOProcessor:\n",
    "    \"\"\"MLOCOMO processor optimized for Google Colab\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.setup_complete = False\n",
    "\n",
    "        # TED Talk configuration\n",
    "        self.video_config = {\n",
    "            \"url\": \"https://www.youtube.com/watch?v=iCvmsMzlF7o\",\n",
    "            \"title\": \"Bren√© Brown: The Power of Vulnerability\",\n",
    "            \"duration\": 1260,  # ~21 minutes\n",
    "            \"license\": \"CC BY-NC-ND 4.0\"\n",
    "        }\n",
    "\n",
    "    def setup_models(self):\n",
    "        \"\"\"Setup models with progress tracking\"\"\"\n",
    "        print(\"üîÑ Setting up models...\")\n",
    "\n",
    "        # Load Whisper\n",
    "        try:\n",
    "            print(\"Loading Whisper model...\")\n",
    "            self.whisper_model = whisper.load_model(\"base\")\n",
    "            print(\"‚úÖ Whisper loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Whisper failed, will use OpenAI API: {e}\")\n",
    "            self.whisper_model = None\n",
    "\n",
    "        # Load sentence transformer\n",
    "        try:\n",
    "            print(\"Loading sentence transformer...\")\n",
    "            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(\"‚úÖ Sentence transformer loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Sentence transformer failed: {e}\")\n",
    "            self.sentence_model = None\n",
    "\n",
    "        self.setup_complete = True\n",
    "        print(\"üéâ Setup complete!\")\n",
    "\n",
    "    def download_video(self) -> str:\n",
    "        \"\"\"Download TED talk video\"\"\"\n",
    "        print(\"üì• Downloading TED talk...\")\n",
    "\n",
    "        output_path = \"ted_talk.mp4\"\n",
    "\n",
    "        cmd = [\n",
    "            \"yt-dlp\",\n",
    "            \"--format\", \"best[height<=720][ext=mp4]/best[ext=mp4]\",\n",
    "            \"--output\", output_path,\n",
    "            self.video_config[\"url\"]\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Show progress\n",
    "            process = subprocess.Popen(\n",
    "                cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True, bufsize=1\n",
    "            )\n",
    "\n",
    "            for line in process.stdout:\n",
    "                if \"%\" in line and \"ETA\" in line:\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"üì• Downloading: {line.strip()}\")\n",
    "\n",
    "            process.wait()\n",
    "\n",
    "            if process.returncode == 0 and os.path.exists(output_path):\n",
    "                print(\"‚úÖ Download complete!\")\n",
    "                return output_path\n",
    "            else:\n",
    "                raise RuntimeError(\"Download failed\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Download failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_audio(self, video_path: str) -> str:\n",
    "        \"\"\"Extract audio from video\"\"\"\n",
    "        print(\"üéµ Extracting audio...\")\n",
    "\n",
    "        audio_path = \"audio.wav\"\n",
    "\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-i\", video_path, \"-vn\",\n",
    "            \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\",\n",
    "            audio_path, \"-y\", \"-loglevel\", \"quiet\"\n",
    "        ]\n",
    "\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(\"‚úÖ Audio extracted!\")\n",
    "        return audio_path\n",
    "\n",
    "    async def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Transcribe audio with timestamps\"\"\"\n",
    "        print(\"üó£Ô∏è  Transcribing audio...\")\n",
    "\n",
    "        if self.whisper_model:\n",
    "            print(\"Using local Whisper...\")\n",
    "            result = self.whisper_model.transcribe(\n",
    "                audio_path, word_timestamps=True, language=\"en\"\n",
    "            )\n",
    "            print(\"‚úÖ Local transcription complete!\")\n",
    "            return result\n",
    "        else:\n",
    "            print(\"Using OpenAI Whisper API...\")\n",
    "            with open(audio_path, \"rb\") as f:\n",
    "                transcript = await self.client.audio.transcriptions.acreate(\n",
    "                    model=\"whisper-1\", file=f, response_format=\"verbose_json\"\n",
    "                )\n",
    "\n",
    "            result = {\n",
    "                \"text\": transcript.text,\n",
    "                \"segments\": []\n",
    "            }\n",
    "\n",
    "            if hasattr(transcript, 'segments'):\n",
    "                for seg in transcript.segments:\n",
    "                    result[\"segments\"].append({\n",
    "                        \"start\": seg.start,\n",
    "                        \"end\": seg.end,\n",
    "                        \"text\": seg.text\n",
    "                    })\n",
    "\n",
    "            print(\"‚úÖ API transcription complete!\")\n",
    "            return result\n",
    "\n",
    "    def identify_speakers(self, transcription: Dict) -> List[Dict]:\n",
    "        \"\"\"Simple speaker identification\"\"\"\n",
    "        print(\"üë• Identifying speakers...\")\n",
    "\n",
    "        segments = transcription.get(\"segments\", [])\n",
    "        speaker_segments = []\n",
    "\n",
    "        for segment in tqdm(segments, desc=\"Processing segments\"):\n",
    "            text = segment.get(\"text\", \"\").lower().strip()\n",
    "            start_time = segment.get(\"start\", 0)\n",
    "\n",
    "            # Simple heuristics\n",
    "            if text.endswith(\"?\") or len(text.split()) < 8:\n",
    "                speaker_id = \"audience_member\"\n",
    "                confidence = 0.7\n",
    "            elif start_time < 60:\n",
    "                speaker_id = \"moderator\"\n",
    "                confidence = 0.6\n",
    "            else:\n",
    "                speaker_id = \"brene_brown\"\n",
    "                confidence = 0.8\n",
    "\n",
    "            speaker_segments.append({\n",
    "                **segment,\n",
    "                \"speaker_id\": speaker_id,\n",
    "                \"speaker_confidence\": confidence\n",
    "            })\n",
    "\n",
    "        print(\"‚úÖ Speaker identification complete!\")\n",
    "        return speaker_segments\n",
    "\n",
    "    def create_segments(self, video_path: str, speaker_segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create 30-second segments with basic analysis\"\"\"\n",
    "        print(\"üé¨ Creating video segments...\")\n",
    "\n",
    "        # Group into 30-second segments\n",
    "        segments = []\n",
    "        current_group = []\n",
    "        current_duration = 0\n",
    "        target_duration = 30.0\n",
    "\n",
    "        for seg in speaker_segments:\n",
    "            seg_duration = seg[\"end\"] - seg[\"start\"]\n",
    "\n",
    "            if current_duration + seg_duration > target_duration and current_group:\n",
    "                # Process current group\n",
    "                segment = self._process_segment_group(video_path, current_group, len(segments))\n",
    "                if segment:\n",
    "                    segments.append(segment)\n",
    "\n",
    "                # Start new group\n",
    "                current_group = [seg]\n",
    "                current_duration = seg_duration\n",
    "            else:\n",
    "                current_group.append(seg)\n",
    "                current_duration += seg_duration\n",
    "\n",
    "        # Process final group\n",
    "        if current_group:\n",
    "            segment = self._process_segment_group(video_path, current_group, len(segments))\n",
    "            if segment:\n",
    "                segments.append(segment)\n",
    "\n",
    "        print(f\"‚úÖ Created {len(segments)} segments!\")\n",
    "        return segments\n",
    "\n",
    "    def _process_segment_group(self, video_path: str, group: List[Dict], segment_idx: int) -> Optional[Dict]:\n",
    "        \"\"\"Process a group of transcription segments into one video segment\"\"\"\n",
    "\n",
    "        if not group:\n",
    "            return None\n",
    "\n",
    "        start_time = group[0][\"start\"]\n",
    "        end_time = group[-1][\"end\"]\n",
    "\n",
    "        # Skip if too short\n",
    "        if end_time - start_time < 10:\n",
    "            return None\n",
    "\n",
    "        # Combine transcript\n",
    "        transcript = \" \".join([seg[\"text\"] for seg in group])\n",
    "\n",
    "        # Determine speaker\n",
    "        speakers = [seg[\"speaker_id\"] for seg in group]\n",
    "        primary_speaker = max(set(speakers), key=speakers.count)\n",
    "        speaker_confidence = np.mean([seg[\"speaker_confidence\"] for seg in group])\n",
    "\n",
    "        # Extract frame for analysis\n",
    "        frame = self._extract_frame(video_path, (start_time + end_time) / 2)\n",
    "\n",
    "        return {\n",
    "            \"segment_id\": f\"seg_{segment_idx:03d}\",\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"transcript\": transcript,\n",
    "            \"speaker_id\": primary_speaker,\n",
    "            \"speaker_confidence\": speaker_confidence,\n",
    "            \"frame\": frame,  # Store for later analysis\n",
    "            \"visual_description\": \"\",  # Will be filled later\n",
    "            \"audio_description\": \"\",   # Will be filled later\n",
    "            \"key_objects\": [],         # Will be filled later\n",
    "            \"scene_type\": \"presentation\"\n",
    "        }\n",
    "\n",
    "    def _extract_frame(self, video_path: str, timestamp: float) -> Optional[np.ndarray]:\n",
    "        \"\"\"Extract frame at timestamp\"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_number = int(timestamp * fps)\n",
    "\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "\n",
    "            return frame if ret else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    async def analyze_segments(self, segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Analyze all segments with multimodal content\"\"\"\n",
    "        print(\"üîç Analyzing multimodal content...\")\n",
    "\n",
    "        for i, segment in enumerate(tqdm(segments, desc=\"Analyzing segments\")):\n",
    "            # Visual analysis\n",
    "            if segment[\"frame\"] is not None:\n",
    "                segment[\"visual_description\"] = await self._analyze_visual(\n",
    "                    segment[\"frame\"], segment[\"transcript\"]\n",
    "                )\n",
    "\n",
    "                # Extract key objects\n",
    "                segment[\"key_objects\"] = await self._extract_key_objects(\n",
    "                    segment[\"visual_description\"], segment[\"transcript\"]\n",
    "                )\n",
    "\n",
    "            # Audio analysis\n",
    "            segment[\"audio_description\"] = await self._analyze_audio(\n",
    "                segment[\"transcript\"], segment[\"start_time\"], segment[\"end_time\"]\n",
    "            )\n",
    "\n",
    "            # Remove frame data (too large for JSON)\n",
    "            del segment[\"frame\"]\n",
    "\n",
    "            # Progress update every 10 segments\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"üîç Analyzed {i}/{len(segments)} segments...\")\n",
    "\n",
    "        print(\"‚úÖ Multimodal analysis complete!\")\n",
    "        return segments\n",
    "\n",
    "    async def _analyze_visual(self, frame: np.ndarray, transcript: str) -> str:\n",
    "        \"\"\"Analyze visual content using GPT-4V\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Encode frame\n",
    "            _, buffer = cv2.imencode('.jpg', frame)\n",
    "            frame_b64 = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"Describe this TED talk frame. Speaker says: '{transcript[:150]}...' \"\n",
    "                                   f\"Focus on: setting, speaker, gestures, slides, audience. Be concise.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame_b64}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                }],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            return \"Visual analysis unavailable\"\n",
    "\n",
    "    async def _analyze_audio(self, transcript: str, start_time: float, end_time: float) -> str:\n",
    "        \"\"\"Analyze audio characteristics\"\"\"\n",
    "\n",
    "        try:\n",
    "            duration = end_time - start_time\n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Describe audio characteristics of this {duration:.1f}s TED talk segment: \"\n",
    "                              f\"'{transcript}' Focus on: pace, tone, emphasis, audience reaction.\"\n",
    "                }],\n",
    "                max_tokens=100\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            return \"Audio analysis unavailable\"\n",
    "\n",
    "    async def _extract_key_objects(self, visual_desc: str, transcript: str) -> List[str]:\n",
    "        \"\"\"Extract key objects\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Extract 3-5 key objects/concepts: Visual: {visual_desc} \"\n",
    "                              f\"Speech: {transcript} Return JSON list of concrete items.\"\n",
    "                }],\n",
    "                max_tokens=80\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except:\n",
    "            return [\"speaker\", \"stage\", \"audience\"]\n",
    "\n",
    "    async def generate_qa_pairs(self, segments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate QA pairs for all recall types and modalities\"\"\"\n",
    "        print(\"‚ùì Generating QA pairs...\")\n",
    "\n",
    "        # Build context\n",
    "        context = \"\\n\".join([\n",
    "            f\"Segment {i}: {seg['transcript'][:100]}... \"\n",
    "            f\"[Visual: {seg['visual_description'][:60]}...] \"\n",
    "            f\"[Speaker: {seg['speaker_id']}]\"\n",
    "            for i, seg in enumerate(segments[:8])\n",
    "        ])\n",
    "\n",
    "        qa_pairs = []\n",
    "        recall_types = [\"single_hop\", \"multi_hop\", \"temporal\", \"commonsense\", \"adversarial\"]\n",
    "        modality_pairs = [\n",
    "            (\"text\", \"text\"), (\"audio\", \"audio\"), (\"image\", \"image\"),\n",
    "            (\"text\", \"audio\"), (\"text\", \"image\"), (\"audio\", \"text\"),\n",
    "            (\"audio\", \"image\"), (\"image\", \"text\"), (\"image\", \"audio\")\n",
    "        ]\n",
    "\n",
    "        total_pairs = len(recall_types) * len(modality_pairs)\n",
    "\n",
    "        with tqdm(total=total_pairs, desc=\"Generating QA pairs\") as pbar:\n",
    "            for recall_type in recall_types:\n",
    "                for source_mod, target_mod in modality_pairs:\n",
    "                    qa_pair = await self._generate_single_qa(\n",
    "                        segments, context, recall_type, source_mod, target_mod\n",
    "                    )\n",
    "                    if qa_pair:\n",
    "                        qa_pairs.append(qa_pair)\n",
    "                    pbar.update(1)\n",
    "\n",
    "        print(f\"‚úÖ Generated {len(qa_pairs)} QA pairs!\")\n",
    "        return qa_pairs\n",
    "\n",
    "    async def _generate_single_qa(self, segments: List[Dict], context: str,\n",
    "                                recall_type: str, source_mod: str, target_mod: str) -> Optional[Dict]:\n",
    "        \"\"\"Generate single QA pair\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Generate a {recall_type} question for Bren√© Brown's TED talk testing {source_mod}‚Üí{target_mod} memory.\n",
    "\n",
    "        Context: {context[:1000]}...\n",
    "\n",
    "        Guidelines:\n",
    "        - {recall_type}: {self._get_recall_description(recall_type)}\n",
    "        - Question tests {source_mod} input requiring {target_mod} knowledge\n",
    "        - Based on vulnerability research content\n",
    "\n",
    "        JSON format:\n",
    "        {{\"question\": \"specific question\", \"answer\": \"accurate answer\", \"difficulty\": \"easy|medium|hard\"}}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = await self.client.chat.completions.acreate(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=200\n",
    "            )\n",
    "\n",
    "            qa_data = json.loads(response.choices[0].message.content)\n",
    "            qa_data.update({\n",
    "                \"recall_type\": recall_type,\n",
    "                \"source_modality\": source_mod,\n",
    "                \"target_modality\": target_mod,\n",
    "                \"requires_cross_modal\": source_mod != target_mod\n",
    "            })\n",
    "\n",
    "            return qa_data\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _get_recall_description(self, recall_type: str) -> str:\n",
    "        descriptions = {\n",
    "            \"single_hop\": \"Direct factual recall from one moment\",\n",
    "            \"multi_hop\": \"Connect information across multiple segments\",\n",
    "            \"temporal\": \"Time-based reasoning about sequence/duration\",\n",
    "            \"commonsense\": \"Inference requiring background knowledge\",\n",
    "            \"adversarial\": \"Edge cases or challenging scenarios\"\n",
    "        }\n",
    "        return descriptions.get(recall_type, \"\")\n",
    "\n",
    "# Initialize processor\n",
    "if OPENAI_API_KEY:\n",
    "    processor = MLOCOMOProcessor(OPENAI_API_KEY)\n",
    "    processor.setup_models()\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without valid API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c015f9",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: LOCOMO Enhanced Processing\n",
    "\n",
    "**This replaces the original processing with LOCOMO-compliant multi-session conversations**\n",
    "\n",
    "Based on [arXiv:2402.17753](https://arxiv.org/abs/2402.17753), this creates:\n",
    "- **35 conversation sessions** (vs. single video processing)\n",
    "- **300+ total turns** across multiple speakers\n",
    "- **150+ QA pairs** (vs. 0 in original)\n",
    "- **Multi-speaker personas** with distinct characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOCOMO ENHANCED PROCESSING - Download and Process Video\n",
    "# =============================================================================\n",
    "\n",
    "# Download video\n",
    "if OPENAI_API_KEY and enhanced_processor.setup_complete:\n",
    "    try:\n",
    "        video_path = enhanced_processor.download_video()\n",
    "        \n",
    "        # Show video info\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        duration = frame_count / fps\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap.release()\n",
    "\n",
    "        print(f\"üìä Video Info:\")\n",
    "        print(f\"   Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
    "        print(f\"   Resolution: {width}x{height}\")\n",
    "        print(f\"   FPS: {fps:.1f}\")\n",
    "        print(f\"   Size: {os.path.getsize(video_path)/1024/1024:.1f} MB\")\n",
    "\n",
    "        # Display first frame\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(frame_rgb)\n",
    "            plt.title(\"First Frame - Bren√© Brown TED Talk\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Video download failed: {e}\")\n",
    "        video_path = None\n",
    "else:\n",
    "    print(\"‚ùå Enhanced processor not ready\")\n",
    "    video_path = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOCOMO ENHANCED PROCESSING - Generate Multi-Session Conversations\n",
    "# =============================================================================\n",
    "\n",
    "if video_path:\n",
    "    try:\n",
    "        print(\"üîÑ Starting LOCOMO enhanced processing...\")\n",
    "        print(\"‚è±Ô∏è  This will take 15-20 minutes...\")\n",
    "        print(\"üìä Expected results:\")\n",
    "        print(\"   - 35 conversation sessions\")\n",
    "        print(\"   - 300+ total conversation turns\")\n",
    "        print(\"   - 150+ QA pairs\")\n",
    "        print(\"   - 3-4 distinct personas\")\n",
    "        \n",
    "        # Process video to LOCOMO format\n",
    "        dataset = enhanced_processor.process_video_to_locomo(video_path)\n",
    "        \n",
    "        print(f\"\\nüéâ LOCOMO Enhanced Processing Complete!\")\n",
    "        print(f\"üìä Final Statistics:\")\n",
    "        print(f\"   Sessions: {dataset['metadata']['total_sessions']}\")\n",
    "        print(f\"   Total turns: {dataset['metadata']['total_turns']}\")\n",
    "        print(f\"   QA pairs: {dataset['metadata']['total_qa_pairs']}\")\n",
    "        print(f\"   Personas: {len(dataset['personas'])}\")\n",
    "        \n",
    "        # Show sample personas\n",
    "        print(f\"\\nüë• Generated Personas:\")\n",
    "        for key, persona in dataset['personas'].items():\n",
    "            print(f\"   {persona['name']} ({persona['background']}): {persona['communication_style']}\")\n",
    "        \n",
    "        # Show sample QA pairs\n",
    "        print(f\"\\n‚ùì Sample QA Pairs:\")\n",
    "        for i, qa in enumerate(dataset['qa_pairs'][:3]):\n",
    "            print(f\"   {i+1}. [{qa['recall_type']}] {qa['question']}\")\n",
    "            print(f\"      Answer: {qa['answer']}\")\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LOCOMO processing failed: {e}\")\n",
    "        dataset = None\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without video\")\n",
    "    dataset = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09214cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE LOCOMO ENHANCED DATASET\n",
    "# =============================================================================\n",
    "\n",
    "if dataset:\n",
    "    try:\n",
    "        # Save to file\n",
    "        output_filename = \"locomo_enhanced_dataset.json\"\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "        \n",
    "        # Calculate file size\n",
    "        file_size = os.path.getsize(output_filename) / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"üíæ LOCOMO Enhanced Dataset saved successfully!\")\n",
    "        print(f\"   Filename: {output_filename}\")\n",
    "        print(f\"   File size: {file_size:.1f} MB\")\n",
    "        print(f\"   Sessions: {dataset['metadata']['total_sessions']}\")\n",
    "        print(f\"   Total turns: {dataset['metadata']['total_turns']}\")\n",
    "        print(f\"   QA pairs: {dataset['metadata']['total_qa_pairs']}\")\n",
    "        print(f\"   Personas: {len(dataset['personas'])}\")\n",
    "        \n",
    "        # Create summary report\n",
    "        summary = f\"\"\"\n",
    "LOCOMO Enhanced Dataset Processing Report\n",
    "========================================\n",
    "\n",
    "Methodology: LOCOMO Enhanced (arXiv:2402.17753)\n",
    "Video Information:\n",
    "- Title: {dataset['metadata']['dataset_name']}\n",
    "- Source: {dataset['metadata']['source_video']}\n",
    "- Processing Date: {dataset['metadata']['processing_timestamp']}\n",
    "\n",
    "Dataset Statistics:\n",
    "- Total Sessions: {dataset['metadata']['total_sessions']}\n",
    "- Total Turns: {dataset['metadata']['total_turns']}\n",
    "- Total QA Pairs: {dataset['metadata']['total_qa_pairs']}\n",
    "- File Size: {file_size:.1f} MB\n",
    "\n",
    "Personas:\n",
    "\"\"\"\n",
    "        \n",
    "        # Add persona details\n",
    "        for key, persona in dataset['personas'].items():\n",
    "            summary += f\"- {persona['name']} ({persona['background']}): {persona['communication_style']}\\n\"\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "QA Distribution by Recall Type:\n",
    "\"\"\"\n",
    "        \n",
    "        # Add QA distribution\n",
    "        recall_types = [qa['recall_type'] for qa in dataset['qa_pairs']]\n",
    "        for recall_type in set(recall_types):\n",
    "            count = recall_types.count(recall_type)\n",
    "            summary += f\"- {recall_type}: {count}\\n\"\n",
    "        \n",
    "        # Save summary\n",
    "        with open(\"locomo_processing_summary.txt\", 'w') as f:\n",
    "            f.write(summary)\n",
    "        \n",
    "        print(f\"\\nüìã Summary report saved to: locomo_processing_summary.txt\")\n",
    "        \n",
    "        # Download files\n",
    "        from google.colab import files\n",
    "        \n",
    "        print(f\"\\nüì• Download your LOCOMO Enhanced files:\")\n",
    "        print(f\"   1. Main dataset: {output_filename}\")\n",
    "        print(f\"   2. Summary report: locomo_processing_summary.txt\")\n",
    "        \n",
    "        # Auto-download files\n",
    "        files.download(output_filename)\n",
    "        files.download(\"locomo_processing_summary.txt\")\n",
    "        \n",
    "        print(f\"\\nüéâ LOCOMO Enhanced processing complete!\")\n",
    "        print(f\"   ‚úÖ {dataset['metadata']['total_sessions']} conversation sessions created\")\n",
    "        print(f\"   ‚úÖ {dataset['metadata']['total_turns']} conversation turns generated\")\n",
    "        print(f\"   ‚úÖ {dataset['metadata']['total_qa_pairs']} QA pairs generated\")\n",
    "        print(f\"   ‚úÖ {len(dataset['personas'])} distinct personas created\")\n",
    "        print(f\"   ‚úÖ Ready for LOCOMO memory evaluation experiments!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Save failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No dataset to save\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198685c8",
   "metadata": {
    "id": "198685c8"
   },
   "source": [
    "## üì• Step 4: Download and Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a49b71",
   "metadata": {
    "id": "01a49b71"
   },
   "outputs": [],
   "source": [
    "# Download TED talk\n",
    "if OPENAI_API_KEY and processor.setup_complete:\n",
    "    try:\n",
    "        video_path = processor.download_video()\n",
    "\n",
    "        # Show video info\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        duration = frame_count / fps\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap.release()\n",
    "\n",
    "        print(f\"üìä Video Info:\")\n",
    "        print(f\"   Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
    "        print(f\"   Resolution: {width}x{height}\")\n",
    "        print(f\"   FPS: {fps:.1f}\")\n",
    "        print(f\"   Size: {os.path.getsize(video_path)/1024/1024:.1f} MB\")\n",
    "\n",
    "        # Display first frame\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(frame_rgb)\n",
    "            plt.title(\"First Frame - Bren√© Brown TED Talk\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Video download failed: {e}\")\n",
    "        video_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edfa07",
   "metadata": {
    "id": "26edfa07"
   },
   "source": [
    "## üéµ Step 5: Audio Processing and Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb05a7b",
   "metadata": {
    "id": "0bb05a7b"
   },
   "outputs": [],
   "source": [
    "if video_path:\n",
    "    try:\n",
    "        # Extract audio\n",
    "        audio_path = processor.extract_audio(video_path)\n",
    "\n",
    "        # Transcribe\n",
    "        transcription = await processor.transcribe_audio(audio_path)\n",
    "\n",
    "        print(f\"üìä Transcription Info:\")\n",
    "        print(f\"   Total text length: {len(transcription['text'])} characters\")\n",
    "        print(f\"   Number of segments: {len(transcription.get('segments', []))}\")\n",
    "\n",
    "        # Show sample transcription\n",
    "        print(f\"\\nüìù Sample transcription:\")\n",
    "        print(f\"'{transcription['text'][:300]}...'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audio processing failed: {e}\")\n",
    "        audio_path = None\n",
    "        transcription = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07913a3e",
   "metadata": {
    "id": "07913a3e"
   },
   "source": [
    "## üë• Step 6: Speaker Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae198cc",
   "metadata": {
    "id": "0ae198cc"
   },
   "outputs": [],
   "source": [
    "if transcription:\n",
    "    try:\n",
    "        speaker_segments = processor.identify_speakers(transcription)\n",
    "\n",
    "        # Analyze speaker distribution\n",
    "        speakers = [seg[\"speaker_id\"] for seg in speaker_segments]\n",
    "        speaker_counts = {\n",
    "            speaker: speakers.count(speaker)\n",
    "            for speaker in set(speakers)\n",
    "        }\n",
    "\n",
    "        print(\"üéØ Speaker Distribution:\")\n",
    "        for speaker, count in speaker_counts.items():\n",
    "            percentage = (count / len(speakers)) * 100\n",
    "            print(f\"   {speaker}: {count} segments ({percentage:.1f}%)\")\n",
    "\n",
    "        # Visualize speaker timeline\n",
    "        fig, ax = plt.subplots(figsize=(15, 4))\n",
    "\n",
    "        colors = {'brene_brown': 'blue', 'moderator': 'green', 'audience_member': 'red'}\n",
    "\n",
    "        for seg in speaker_segments:\n",
    "            speaker = seg[\"speaker_id\"]\n",
    "            start = seg[\"start\"]\n",
    "            duration = seg[\"end\"] - seg[\"start\"]\n",
    "            ax.barh(speaker, duration, left=start, color=colors.get(speaker, 'gray'), alpha=0.7)\n",
    "\n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_title('Speaker Timeline')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Speaker identification failed: {e}\")\n",
    "        speaker_segments = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c8934",
   "metadata": {
    "id": "6b6c8934"
   },
   "source": [
    "## üé¨ Step 7: Create Video Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e84a77",
   "metadata": {
    "id": "c2e84a77"
   },
   "outputs": [],
   "source": [
    "if speaker_segments:\n",
    "    try:\n",
    "        segments = processor.create_segments(video_path, speaker_segments)\n",
    "\n",
    "        print(f\"üìä Segment Statistics:\")\n",
    "        print(f\"   Total segments: {len(segments)}\")\n",
    "\n",
    "        if segments:\n",
    "            durations = [seg[\"end_time\"] - seg[\"start_time\"] for seg in segments]\n",
    "            print(f\"   Average duration: {np.mean(durations):.1f} seconds\")\n",
    "            print(f\"   Duration range: {np.min(durations):.1f} - {np.max(durations):.1f} seconds\")\n",
    "\n",
    "            # Show segment distribution\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Duration histogram\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(durations, bins=20, alpha=0.7, color='skyblue')\n",
    "            plt.xlabel('Segment Duration (seconds)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Segment Duration Distribution')\n",
    "\n",
    "            # Speaker distribution in segments\n",
    "            plt.subplot(1, 2, 2)\n",
    "            segment_speakers = [seg[\"speaker_id\"] for seg in segments]\n",
    "            speaker_counts = {s: segment_speakers.count(s) for s in set(segment_speakers)}\n",
    "            plt.pie(speaker_counts.values(), labels=speaker_counts.keys(), autopct='%1.1f%%')\n",
    "            plt.title('Speaker Distribution in Segments')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        print(\"‚úÖ Video segmentation complete!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Segmentation failed: {e}\")\n",
    "        segments = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddebe81",
   "metadata": {
    "id": "1ddebe81"
   },
   "source": [
    "## üîç Step 8: Multimodal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a5a08",
   "metadata": {
    "id": "c83a5a08"
   },
   "outputs": [],
   "source": [
    "if segments:\n",
    "    try:\n",
    "        print(\"üîÑ Starting multimodal analysis...\")\n",
    "        print(\"‚è±Ô∏è  This will take 5-10 minutes...\")\n",
    "\n",
    "        analyzed_segments = await processor.analyze_segments(segments)\n",
    "\n",
    "        print(f\"‚úÖ Multimodal analysis complete!\")\n",
    "        print(f\"   Analyzed {len(analyzed_segments)} segments\")\n",
    "\n",
    "        # Show sample analysis\n",
    "        if analyzed_segments:\n",
    "            sample = analyzed_segments[0]\n",
    "            print(f\"\\nüìã Sample Analysis (Segment 0):\")\n",
    "            print(f\"   Transcript: {sample['transcript'][:100]}...\")\n",
    "            print(f\"   Visual: {sample['visual_description'][:100]}...\")\n",
    "            print(f\"   Audio: {sample['audio_description'][:100]}...\")\n",
    "            print(f\"   Key Objects: {sample['key_objects']}\")\n",
    "            print(f\"   Speaker: {sample['speaker_id']} ({sample['speaker_confidence']:.2f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Multimodal analysis failed: {e}\")\n",
    "        analyzed_segments = segments  # Use basic segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7467740",
   "metadata": {
    "id": "b7467740"
   },
   "source": [
    "## ‚ùì Step 9: Generate QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f9735",
   "metadata": {
    "id": "570f9735"
   },
   "outputs": [],
   "source": [
    "if analyzed_segments:\n",
    "    try:\n",
    "        print(\"üîÑ Generating QA pairs...\")\n",
    "        print(\"‚è±Ô∏è  This will take 8-12 minutes...\")\n",
    "\n",
    "        qa_pairs = await processor.generate_qa_pairs(analyzed_segments)\n",
    "\n",
    "        print(f\"‚úÖ QA generation complete!\")\n",
    "        print(f\"   Generated {len(qa_pairs)} QA pairs\")\n",
    "\n",
    "        # Analyze QA distribution\n",
    "        recall_types = [qa[\"recall_type\"] for qa in qa_pairs]\n",
    "        modality_pairs = [f\"{qa['source_modality']}‚Üí{qa['target_modality']}\" for qa in qa_pairs]\n",
    "\n",
    "        print(f\"\\nüìä QA Distribution:\")\n",
    "\n",
    "        # Recall type distribution\n",
    "        recall_counts = {rt: recall_types.count(rt) for rt in set(recall_types)}\n",
    "        print(f\"   By recall type:\")\n",
    "        for rt, count in recall_counts.items():\n",
    "            print(f\"      {rt}: {count}\")\n",
    "\n",
    "        # Cross-modal vs unimodal\n",
    "        cross_modal = sum(1 for qa in qa_pairs if qa[\"requires_cross_modal\"])\n",
    "        unimodal = len(qa_pairs) - cross_modal\n",
    "        print(f\"   Cross-modal: {cross_modal}, Unimodal: {unimodal}\")\n",
    "\n",
    "        # Show sample QA pairs\n",
    "        print(f\"\\nüìù Sample QA Pairs:\")\n",
    "        for i, qa in enumerate(qa_pairs[:3]):\n",
    "            print(f\"   {i+1}. [{qa['recall_type']}] {qa['question']}\")\n",
    "            print(f\"      Answer: {qa['answer']}\")\n",
    "            print(f\"      Modality: {qa['source_modality']}‚Üí{qa['target_modality']}\")\n",
    "            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå QA generation failed: {e}\")\n",
    "        qa_pairs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76215ce",
   "metadata": {
    "id": "d76215ce"
   },
   "source": [
    "## üíæ Step 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723454b2",
   "metadata": {
    "id": "723454b2"
   },
   "outputs": [],
   "source": [
    "if analyzed_segments and qa_pairs:\n",
    "    try:\n",
    "        # Create final dataset\n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"source_video\": processor.video_config[\"url\"],\n",
    "                \"title\": processor.video_config[\"title\"],\n",
    "                \"license\": processor.video_config[\"license\"],\n",
    "                \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_segments\": len(analyzed_segments),\n",
    "                \"total_qa_pairs\": len(qa_pairs),\n",
    "                \"processing_tool\": \"Google Colab MLOCOMO Processor\"\n",
    "            },\n",
    "            \"segments\": analyzed_segments,\n",
    "            \"qa_pairs\": qa_pairs\n",
    "        }\n",
    "\n",
    "        # Save to file\n",
    "        output_filename = \"brene_brown_mlocomo_dataset.json\"\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "\n",
    "        # Calculate file size\n",
    "        file_size = os.path.getsize(output_filename) / 1024 / 1024  # MB\n",
    "\n",
    "        print(f\"üíæ Dataset saved successfully!\")\n",
    "        print(f\"   Filename: {output_filename}\")\n",
    "        print(f\"   File size: {file_size:.1f} MB\")\n",
    "        print(f\"   Segments: {len(analyzed_segments)}\")\n",
    "        print(f\"   QA pairs: {len(qa_pairs)}\")\n",
    "\n",
    "        # Create summary report\n",
    "        summary = f\"\"\"\n",
    "MLOCOMO Dataset Processing Report\n",
    "================================\n",
    "\n",
    "Video Information:\n",
    "- Title: {dataset['metadata']['title']}\n",
    "- Source: {dataset['metadata']['source_video']}\n",
    "- License: {dataset['metadata']['license']}\n",
    "- Processing Date: {dataset['metadata']['processing_timestamp']}\n",
    "\n",
    "Dataset Statistics:\n",
    "- Total Segments: {len(analyzed_segments)}\n",
    "- Total QA Pairs: {len(qa_pairs)}\n",
    "- File Size: {file_size:.1f} MB\n",
    "\n",
    "Speaker Distribution:\n",
    "\"\"\"\n",
    "\n",
    "        # Add speaker distribution\n",
    "        speakers = [seg[\"speaker_id\"] for seg in analyzed_segments]\n",
    "        for speaker in set(speakers):\n",
    "            count = speakers.count(speaker)\n",
    "            percentage = (count / len(speakers)) * 100\n",
    "            summary += f\"- {speaker}: {count} segments ({percentage:.1f}%)\\n\"\n",
    "\n",
    "        summary += f\"\"\"\n",
    "QA Pair Distribution:\n",
    "\"\"\"\n",
    "\n",
    "        # Add QA distribution\n",
    "        for recall_type in set(recall_types):\n",
    "            count = recall_types.count(recall_type)\n",
    "            summary += f\"- {recall_type}: {count}\\n\"\n",
    "\n",
    "        # Save summary\n",
    "        with open(\"processing_summary.txt\", 'w') as f:\n",
    "            f.write(summary)\n",
    "\n",
    "        print(f\"\\nüìã Summary report saved to: processing_summary.txt\")\n",
    "\n",
    "        # Display download links\n",
    "        from google.colab import files\n",
    "\n",
    "        print(f\"\\nüì• Download your files:\")\n",
    "        print(f\"   1. Main dataset: {output_filename}\")\n",
    "        print(f\"   2. Summary report: processing_summary.txt\")\n",
    "\n",
    "        # Auto-download files\n",
    "        files.download(output_filename)\n",
    "        files.download(\"processing_summary.txt\")\n",
    "\n",
    "        print(f\"\\nüéâ MLOCOMO processing complete!\")\n",
    "        print(f\"   ‚úÖ {len(analyzed_segments)} multimodal segments created\")\n",
    "        print(f\"   ‚úÖ {len(qa_pairs)} QA pairs generated\")\n",
    "        print(f\"   ‚úÖ Ready for memory evaluation experiments!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Save failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59721c3",
   "metadata": {
    "id": "f59721c3"
   },
   "source": [
    "## üìä Step 11: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eb6ca",
   "metadata": {
    "id": "678eb6ca"
   },
   "outputs": [],
   "source": [
    "if analyzed_segments and qa_pairs:\n",
    "\n",
    "    # 1. Segment timeline with multimodal content\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "    # Timeline of segments\n",
    "    for i, seg in enumerate(analyzed_segments):\n",
    "        start = seg[\"start_time\"]\n",
    "        duration = seg[\"end_time\"] - seg[\"start_time\"]\n",
    "        speaker = seg[\"speaker_id\"]\n",
    "\n",
    "        color_map = {'brene_brown': 'blue', 'moderator': 'green', 'audience_member': 'red'}\n",
    "        color = color_map.get(speaker, 'gray')\n",
    "\n",
    "        ax1.barh(0, duration, left=start, color=color, alpha=0.7, height=0.5)\n",
    "\n",
    "    ax1.set_xlabel('Time (seconds)')\n",
    "    ax1.set_title('Video Segment Timeline by Speaker')\n",
    "    ax1.set_ylim(-0.5, 0.5)\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    # QA pair distribution heatmap\n",
    "    recall_types = [\"single_hop\", \"multi_hop\", \"temporal\", \"commonsense\", \"adversarial\"]\n",
    "    modalities = [\"text\", \"audio\", \"image\"]\n",
    "\n",
    "    # Create matrix for cross-modal pairs\n",
    "    matrix = np.zeros((len(modalities), len(modalities)))\n",
    "\n",
    "    for qa in qa_pairs:\n",
    "        source_idx = modalities.index(qa[\"source_modality\"])\n",
    "        target_idx = modalities.index(qa[\"target_modality\"])\n",
    "        matrix[source_idx][target_idx] += 1\n",
    "\n",
    "    im = ax2.imshow(matrix, cmap='Blues')\n",
    "    ax2.set_xticks(range(len(modalities)))\n",
    "    ax2.set_yticks(range(len(modalities)))\n",
    "    ax2.set_xticklabels(modalities)\n",
    "    ax2.set_yticklabels(modalities)\n",
    "    ax2.set_xlabel('Target Modality')\n",
    "    ax2.set_ylabel('Source Modality')\n",
    "    ax2.set_title('QA Pairs by Modality Transfer')\n",
    "\n",
    "    # Add text annotations\n",
    "    for i in range(len(modalities)):\n",
    "        for j in range(len(modalities)):\n",
    "            text = ax2.text(j, i, int(matrix[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    plt.colorbar(im, ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Sample frame visualization\n",
    "    print(\"üñºÔ∏è  Sample Frames and Analysis:\")\n",
    "\n",
    "    if video_path and len(analyzed_segments) >= 3:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        for i, seg_idx in enumerate([0, len(analyzed_segments)//2, -1]):\n",
    "            seg = analyzed_segments[seg_idx]\n",
    "            timestamp = (seg[\"start_time\"] + seg[\"end_time\"]) / 2\n",
    "\n",
    "            # Extract frame\n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC, timestamp * 1000)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                axes[i].imshow(frame_rgb)\n",
    "                axes[i].set_title(f\"Segment {seg_idx}: {seg['speaker_id']}\\n{seg['transcript'][:50]}...\")\n",
    "                axes[i].axis('off')\n",
    "\n",
    "        cap.release()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e2935",
   "metadata": {
    "id": "e25e2935"
   },
   "source": [
    "## üéØ Next Steps\n",
    "#\n",
    "# üìä **Dataset Overview:**\n",
    "# - **Segments:** ~74 multimodal video segments\n",
    "# - **QA Pairs:** ~45 memory evaluation questions\n",
    "# - **Modalities:** Text, Audio, Visual content\n",
    "# - **Recall Types:** Single-hop, Multi-hop, Temporal, Commonsense, Adversarial\n",
    "#\n",
    "# üî¨ **For Research:**\n",
    "# 1. **Evaluate MLLMs:** Test GPT-4V, Gemini, Claude on your QA pairs\n",
    "# 2. **Compare Memory Performance:** Use the evaluation metrics from LOCOMO paper\n",
    "# 3. **Cross-Modal Analysis:** Study how information transfers between modalities\n",
    "# 4. **Extend the Framework:** Add more videos or recall types\n",
    "#\n",
    "#  üí° **Integration:**\n",
    "# - Load the JSON dataset into your evaluation pipeline\n",
    "# - Use the segment timestamps for video-based evaluation\n",
    "# - Combine with the evaluation framework for systematic testing\n",
    "#\n",
    "# **Cost Summary:** ~$3 in OpenAI API calls for a complete research-grade dataset!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
